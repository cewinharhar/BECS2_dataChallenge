{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BECS 2 Data Challenge: Predicting Cancer Status of Patients\n",
    "Koch Kilian (ZHAW), Rieder Jonathan (ZHAW), Yar Kevin (ZHAW)\n",
    "\n",
    "The aim of this project work is to predict 6 different healt status of patients (healthy, breast-, lung-, pancreatic-, colorectal- and prostate cancer). The prediction is based on mass spectroscopy data of different protein levels meassured in patients blood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading packages and define initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nusage joblib\\n# save the model: \\njoblib.dump(model , \"model.pkl\")\\n# load the model:\\nmodel = joblib.load(\"model.pkl\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary packages\n",
    "import os\n",
    "import random\n",
    "import xgboost\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SequentialFeatureSelector \n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "os.getcwd()\n",
    "\"\"\"\n",
    "usage joblib\n",
    "# save the model: \n",
    "joblib.dump(model , \"model.pkl\")\n",
    "# load the model:\n",
    "model = joblib.load(\"model.pkl\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprocessing\n",
    "In this section the raw data is preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kilian insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Preparation\n",
    "In this section the raw data is prepared and reshaped to be fed into the different models. Furhtermore, the distribution of the input data is visualized to check if the data set is balanced. The data is converted into two main variables X (patiens and the coresponding protein quantities) and y (patients and the coresponding healt condition).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pandas dataframe\n",
    "path = \"tidy.csv\"\n",
    "pathMet = \"metadata.csv\"\n",
    "tidy = pd.read_csv(path, sep=\",\")\n",
    "tidyMet = pd.read_csv(pathMet, sep=\";\", index_col=0)\n",
    "\n",
    "#remove samples which are not in the metadata index column (quality controle etc)\n",
    "tidy = tidy[ (tidy[\"R.FileName\"].isin(tidyMet.index)) ]\n",
    "tidyMer    = pd.merge(tidy, tidyMet, how=\"left\", on=\"R.FileName\")\n",
    "tidySub = tidyMer[[\"R.FileName\", \"uniprot\", \"meanAbu\", \"Cancer\"]]\n",
    "tidySub.Cancer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data for model\n",
    "#X data\n",
    "tidyReshaped = tidySub.pivot(index = \"R.FileName\", columns = \"uniprot\", values = \"meanAbu\")\n",
    "tidyReshaped.head()\n",
    "\n",
    "#y condition\n",
    "Group =  tidySub.drop([\"uniprot\", \"meanAbu\"], axis=1)\n",
    "Group = Group.drop_duplicates().reset_index(drop=True)\n",
    "Group.head()\n",
    "\n",
    "#merge X and y and set dataframe to numerical values\n",
    "data = pd.merge(tidyReshaped, Group, how=\"left\", on=\"R.FileName\")\n",
    "data = data.set_index(\"R.FileName\")\n",
    "\n",
    "X_ = data.iloc[:, :-1].apply(np.log2)\n",
    "y_ = data.iloc[:,-1]\n",
    "\n",
    "#check first 10 entries of the dataframe \n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(30, 5))\n",
    "\n",
    "ax.boxplot(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Creation of Data Sets, Feature and Model Selection\n",
    "* Create pipeline for imputing, scaling !! **Scaling is not needed for Random Forest**\n",
    "* (https://towardsdatascience.com/how-data-normalization-affects-your-random-forest-algorithm-fbc6753b4ddf)\n",
    "* Creation of training, validation and test sets\n",
    "* Feature Selection, Engineering\n",
    "* Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make pipeline\n",
    "dataPrepPipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ]) \n",
    "\n",
    "#X is already purely numerical\n",
    "X = dataPrepPipe.fit_transform(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "labEnc = preprocessing.LabelEncoder() \n",
    "y = labEnc.fit_transform(y_) \n",
    "set(zip(y_, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X and y for next session\n",
    "joblib.dump(y, \"Models/y.pkl\")\n",
    "joblib.dump(X, \"Models/X.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved X and y \n",
    "y = joblib.load(\"Models/y.pkl\")\n",
    "X = joblib.load(\"Models/X.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "### Two possibilities:\n",
    "* Tree-based feature selection - sklearn.feature_selection.SelectFromModel\n",
    "    * May be used with sklearn.tree models\n",
    "##\n",
    "* Sequential Feature Selection - sklearn.feature_selection.SequentialFeatureSelector\n",
    "    * May be used with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Feature Selection \n",
    "\n",
    "\"\"\"\n",
    "Can be used in pipeline\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "\"\"\"\n",
    "#create Random Forest classifier with default hyperparameters\n",
    "raFo = RandomForestClassifier(random_state=1)\n",
    "raFo = raFo.fit(X, y)\n",
    "\n",
    "#checkout importance in a histogram\n",
    "plt.hist(raFo.feature_importances_, bins=100)\n",
    "plt.title(\"Histogram of the feature importance for all 2730 proteins\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "\n",
    "#get the reduced X\n",
    "model = SelectFromModel(estimator = raFo, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "print(f\"Original X shape: {X.shape}\")\n",
    "print(f\"Feature selected X_new shape: {X_new.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential Feature Selection \n",
    "\n",
    "params = dict(tree_method=\"exact\", \n",
    "                eval_metric='mlogloss',\n",
    "                use_label_encoder =False)\n",
    "\n",
    "clf_XGRF = xgboost.XGBClassifier(random_state=7, **params)\n",
    "model = SequentialFeatureSelector(estimator = clf_XGRF, n_features_to_select = 0.20, cv = 10,  n_jobs=-1)\n",
    "model.fit(X,y)\n",
    "X_new = model.transform(X)\n",
    "#checkout importance in a histogram\n",
    "plt.hist(raFo.feature_importances_, bins=100)\n",
    "\n",
    "print(f\"Original X shape: {X.shape}\")\n",
    "print(f\"Feature selected X_new shape: {X_new.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "\n",
    "importances = raFo.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in raFo.estimators_], axis=0)\n",
    "std.sort()\n",
    "\n",
    "forest_importances = pd.Series(importances)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "ax.get_xaxis().set_visible(False)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import already trained model\n",
    "\n",
    "clf_RF = joblib.load(\"Models/clf_RF_X_new.pkl\")\n",
    "clf_XGRF = joblib.load(\"Models/clf_XGRF_X_new.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=4)\n",
    "clf_RF = RandomForestClassifier(random_state=1)\n",
    "clf_RF.fit(X_train ,y_train)\n",
    "y_RFpred = clf_RF.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy Random Forest:\",metrics.accuracy_score(y_test , y_RFpred))\n",
    "print(classification_report(y_test, y_RFpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some parameters for xgboost to avoid warnings\n",
    "params = dict(tree_method=\"exact\", \n",
    "                eval_metric='mlogloss',\n",
    "                use_label_encoder =False)\n",
    "\n",
    "clf_XGRF = xgboost.XGBClassifier(random_state=7, **params)\n",
    "\n",
    "clf_XGRF.fit(X_train ,y_train)\n",
    "y_XGRFpred = clf_XGRF.predict(X_test)\n",
    "\n",
    "print(\"Accuracy XGBoost Random Forest:\",metrics.accuracy_score(y_test , y_XGRFpred))\n",
    "print(classification_report(y_test, y_XGRFpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "joblib.dump(clf_RF, \"Models/clf_RF_X_new.pkl\")\n",
    "joblib.dump(clf_XGRF, \"Models/clf_XGRF_X_new.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Hyperparameter Tuning\n",
    "In this chapter the model is optimized by hyperparameter tuning. A random grid search is applied to selected hyperparameters of both models. The hyperparametertuning follows the instructions of the following publication https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 (31.12.21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for the random forest classifier using random grid search\n",
    "#get model parameters(delete after hyperparameter tuning)\n",
    "#params = clf_RF.get_params()\n",
    "#params\n",
    "\n",
    "#Define hyperparameters for tuning\n",
    "n_estimators_RF = [int(x) for x in np.linspace(start=200, stop=2000, num=10)] #number of trees\n",
    "max_features_RF = ['auot', 'sqrt'] #number of features\n",
    "max_depth_RF = [int(x) for x in np.linspace(10, 110, num=1)] #numbber of levels\n",
    "max_depth_RF.append(None)\n",
    "min_samples_split_RF = [2, 5, 10] #minimum number of samples required to split a note\n",
    "min_samples_leaf_RF = [1, 2, 4] #minimum number of samples required at each leaf node\n",
    "bootstrap_RF = [True, False] #method of selecting samples for training\n",
    "\n",
    "#initialize random grid\n",
    "random_grid = {'n_estimators': n_estimators_RF,\n",
    "            'max_features': max_features_RF,\n",
    "            'max_depth': max_depth_RF,\n",
    "            'min_samples_split': min_samples_split_RF,\n",
    "            'min_samples_leaf': min_samples_leaf_RF,\n",
    "            'bootstrap': bootstrap_RF}\n",
    "\n",
    "#define model parameters for random grid search\n",
    "RF_random = RandomizedSearchCV(estimator = clf_RF,\n",
    "            param_distributions=random_grid, n_iter=200,\n",
    "            cv=5, verbose=2, random_state=13, n_jobs=-1)\n",
    "\n",
    "#fit the random search model\n",
    "RF_random.fit(X_train, y_train)\n",
    "\n",
    "#get best hyperparameters from the model\n",
    "RF_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model with optimized hyperparameters\n",
    "clf_RF_tuned = RandomForestClassifier(n_estimators=400,\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=4,\n",
    "                                    max_features='sqrt',\n",
    "                                    max_depth= None,\n",
    "                                    bootstrap= True,\n",
    "                                    random_state=1)\n",
    "clf_RF_tuned.fit(X_train ,y_train)\n",
    "y_RFpred_tuned = clf_RF_tuned.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy Random Forest:\",metrics.accuracy_score(y_test , y_RFpred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = clf_RF.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for the random forest classifier using random grid search\n",
    "#get model parameters(delete after hyperparameter tuning)\n",
    "params = clf_XGRF.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier()\n",
    "# second try hyperparameter tuning\n",
    "\n",
    "#Hyperparameter tuning for the random forest classifier using random grid search\n",
    "#get model parameters(delete after hyperparameter tuning)\n",
    "#params = clf_RF.get_params()\n",
    "#params\\n,\n",
    "\n",
    "#Define hyperparameters for tuning\\n,\n",
    "n_estimators_RF = [x for x in np.linspace(start=50, stop=500, num=10, dtype=int)] #number of trees\n",
    "criterion_RF    = [\"gini\", \"entropy\"]\n",
    "\n",
    "max_depth_RF = [int(x) for x in np.arange(1, 20)] #numbber of levels\n",
    "max_depth_RF.append(None)\n",
    "\n",
    "\n",
    "min_samples_split_RF = [1, 2, 5, 10] #minimum number of samples required to split a note\n",
    "min_samples_leaf_RF = [np.arange(start=1, stop=5)] #minimum number of samples required at each leaf node\n",
    "\n",
    "max_features_RF = ['auto', 'sqrt', \"log2\"] #number of features\n",
    "\n",
    "class_weight        = []  #<---     SUPER WICHTIG FÜR BIOMARKER\n",
    "\n",
    "bootstrap_RF = [True, False] #method of selecting samples for training\n",
    "#initialize random grid \\n,\n",
    "# RANDOM FOREST PARAMS\\n,\n",
    "random_grid = {'n_estimators'   : n_estimators_RF,\n",
    "            'max_features'      : max_features_RF,\n",
    "            'max_depth'         : max_depth_RF,                        \n",
    "            'min_samples_split' : min_samples_split_RF,\n",
    "            'bootstrap'         : bootstrap_RF}\n",
    "\n",
    "#define model parameters for random grid search\n",
    "RF_random = RandomizedSearchCV(estimator = clf_RF,\n",
    "            param_distributions=random_grid, n_iter=100,\n",
    "            cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "#fit the random search model\n",
    "RF_random.fit(X_train, y_train)\n",
    "\n",
    "#get best hyperparameters from the model\n",
    "RF_random.best_params\n",
    "\n",
    "\n",
    "\n",
    "params = {'n_estimators': 350,\n",
    "    'min_samples_split': 2,\n",
    "    'max_features': 'sqrt',\n",
    "    'max_depth': None,\n",
    "    'bootstrap': False}\n",
    "\n",
    "clf_RF = RandomForestClassifier(**params)\n",
    "clf_RF.fit(X_train, y_train) \n",
    "\n",
    "y_RFpred = clf_RF.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\\n,\n",
    "print(\"Accuracy Random Forest: \",metrics.accuracy_score(y_test , y_RFpred))\n",
    "print(classification_report(y_test, y_RFpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_RFpred, labels=clf_RF.classes_) # calculate value\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,              # display\n",
    "                              display_labels=clf_RF.classes_)\n",
    "disp.plot(); \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare uncertainty of Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf_RF, X_new, y, cv=5, scoring='accuracy')\n",
    "Udata = scores.std()\n",
    "\n",
    "modAcuRF = []\n",
    "for rs in range(1,6):\n",
    "    model = RandomForestClassifier(random_state=random.randrange(rs))\n",
    "    model.fit(X_train, y_train)\n",
    "    modAcuRF += [accuracy_score(y_test, model.predict(X_test))]\n",
    "\n",
    "Umodel = np.std(modAcuRF)\n",
    "\n",
    "print(\"Uncertainty in the data: %.3f\" % Udata)\n",
    "print(\"Uncertainty in the model: %.3f\" % Umodel)\n",
    "print(\"The model performance is %.3f ± %.3f ± %.3f\" % (scores.mean(),Udata,Umodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf_XGRF, X_new, y, cv=5, scoring='accuracy')\n",
    "Udata = scores.std()\n",
    "\n",
    "modAcuXGRF = []\n",
    "for rs in range(1,6):\n",
    "    model = xgboost.XGBClassifier(random_state=random.randrange(rs), **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    modAcuXGRF += [accuracy_score(y_test, model.predict(X_test))]\n",
    "\n",
    "Umodel = np.std(modAcuXGRF)\n",
    "\n",
    "print(\"Uncertainty in the data: %.3f\" % Udata)\n",
    "print(\"Uncertainty in the model: %.3f\" % Umodel)\n",
    "print(\"The model performance is %.3f ± %.3f ± %.3f\" % (scores.mean(),Udata,Umodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43fef3abedee1d893beac1d2d118646f520aa845e2b69655f65decb73f4f0136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
